{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to write an AutoTVM template?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to show how to convert the schedule we just created into an AutoTVM template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import logging\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm import autotvm\n",
    "\n",
    "# The size of the matrix\n",
    "# (M, K) x (K, N)\n",
    "# You are free to try out different shapes, sometimes TVM optimization outperforms numpy with MKL.\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "# The default tensor type in tvm\n",
    "dtype = \"float32\"\n",
    "\n",
    "# using Intel AVX2(Advanced Vector Extensions) ISA for SIMD\n",
    "# To get the best performance, please change the following line\n",
    "# to llvm -mcpu=core-avx2, or specific type of CPU you use\n",
    "target = 'llvm -mcpu=core-avx2'\n",
    "ctx = tvm.context(target, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a manual schedule for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_v0(M, K, N):\n",
    "    A = te.placeholder((M, K), name='A')\n",
    "    B = te.placeholder((K, N), name='B')\n",
    "    k = te.reduce_axis((0, K), 'k')\n",
    "    C = te.compute((M, N),\n",
    "                   lambda y, x: te.sum(A[y, k] * B[k, x], axis=k),\n",
    "                   name='C')\n",
    "\n",
    "    # schedule\n",
    "    s = te.create_schedule(C.op)\n",
    "\n",
    "    bn = 32\n",
    "    y, x = s[C].op.axis\n",
    "    k, = s[C].op.reduce_axis\n",
    "    yo, yi = s[C].split(y, bn)\n",
    "    xo, xi = s[C].split(x, bn)\n",
    "    ko, ki = s[C].split(k, 4)\n",
    "\n",
    "    # re-ordering\n",
    "    s[C].reorder(yo, xo, ko, yi, ki, xi)\n",
    "    s[C].vectorize(xi)\n",
    "    s[C].parallel(yo)\n",
    "\n",
    "    return s, [A, B, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous schedule code, we use constant 32 and 8 as tiling factors. However, it might not be the best ones on your hardware.\n",
    "\n",
    "To solve this problem, we can convert the constants into a tunable parameter and let AutoTVM to learn which value works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. use a decorator\n",
    "@autotvm.template(\"example/matmul_v1\")\n",
    "def matmul_v1(M, K, N):\n",
    "    A = te.placeholder((M, K), name='A')\n",
    "    B = te.placeholder((K, N), name='B')\n",
    "    k = te.reduce_axis((0, K), 'k')\n",
    "    C = te.compute((M, N),\n",
    "                   lambda y, x: te.sum(A[y, k] * B[k, x], axis=k),\n",
    "                   name='C')\n",
    "\n",
    "    # schedule\n",
    "    s = te.create_schedule(C.op)\n",
    "    y, x = s[C].op.axis\n",
    "    k, = s[C].op.reduce_axis\n",
    "    \n",
    "    # 2. get the config object\n",
    "    cfg = autotvm.get_config()\n",
    "    \n",
    "    # 3. define search space\n",
    "    cfg.define_split(\"tile_y\", M, num_outputs=2)\n",
    "    cfg.define_split(\"tile_x\", N, num_outputs=2, filter=lambda x: x.size[1] in [1, 2, 4, 8, 16])\n",
    "    cfg.define_split(\"tile_k\", K, num_outputs=2)\n",
    "    \n",
    "    # 4. apply the config\n",
    "    yo, yi = cfg[\"tile_y\"].apply(s, C, y)\n",
    "    xo, xi = cfg[\"tile_x\"].apply(s, C, x)\n",
    "    ko, ki = cfg[\"tile_k\"].apply(s, C, k)\n",
    "\n",
    "    # 5. Finish the rest of schedule\n",
    "    s[C].reorder(yo, xo, ko, yi, ki, xi)\n",
    "    s[C].vectorize(xi)\n",
    "    s[C].parallel(yo)\n",
    "\n",
    "    return s, [A, B, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the a tuning task, and we can inspect the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigSpace (len=605, space_map=\n",
      "   0 tile_y: Split(policy=factors, product=1024, num_outputs=2) len=11\n",
      "   1 tile_x: Split(policy=factors, product=1024, num_outputs=2) len=5\n",
      "   2 tile_k: Split(policy=factors, product=1024, num_outputs=2) len=11\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "task = autotvm.task.create(\"example/matmul_v1\", args=(M, K, N), target='llvm -mcpu=core-avx2')\n",
    "print(task.config_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four different tuners predefined in AutoTVM: random, grid search, genetic algorithm, and XGBoost. Usually we use XGBoost tuner to tune the task. In this tutorial, since the search space is small, we use random tuner to tune 10 iterations for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging config (for printing tuning log to the screen)\n",
    "logging.getLogger('autotvm').setLevel(logging.DEBUG)\n",
    "logging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# There are two steps for measuring a config: build and run.\n",
    "# By default, we use all CPU cores to compile program. Then measure them sequentially.\n",
    "# We measure 5 times and take average to reduce variance.\n",
    "measure_option = autotvm.measure_option(\n",
    "    builder='local',\n",
    "    runner=autotvm.LocalRunner(number=5))\n",
    "\n",
    "# Begin tuning with RandomTuner, log records to file `matmul.log`\n",
    "# You can use alternatives like XGBTuner.\n",
    "tuner = autotvm.tuner.RandomTuner(task)\n",
    "tuner.tune(n_trial=10,\n",
    "           measure_option=measure_option,\n",
    "           callbacks=[autotvm.callback.log_to_file('matmul.log')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "```\n",
    "No: 1\tGFLOPS: 70.62/70.62\tresult: MeasureResult(costs=(0.030408550200000002,), error_no=0, all_cost=0.9211561679840088, timestamp=1587486930.823651)\t[('tile_y', [-1, 32]), ('tile_x', [-1, 128]), ('tile_k', [-1, 8])],None,445\n",
    "No: 2\tGFLOPS: 29.38/70.62\tresult: MeasureResult(costs=(0.07308577720000001,), error_no=0, all_cost=1.4772090911865234, timestamp=1587486932.216256)\t[('tile_y', [-1, 8]), ('tile_x', [-1, 16]), ('tile_k', [-1, 4])],None,289\n",
    "No: 3\tGFLOPS: 43.27/70.62\tresult: MeasureResult(costs=(0.0496339182,), error_no=0, all_cost=1.098529577255249, timestamp=1587486933.2311969)\t[('tile_y', [-1, 64]), ('tile_x', [-1, 8]), ('tile_k', [-1, 8])],None,402\n",
    "No: 4\tGFLOPS: 0.00/70.62\tresult: MeasureResult(costs=(RuntimeError(\"Traceback (most recent call last):\\n  [bt] (5) 6   ???                                 0x00007ffee33395f0 0x0 + 140732710229488\\n  [bt] (4) 5   libffi.6.dylib                      0x000000010f1e4884 ffi_call_unix64 + 76\\n  [bt] (3) 4   libtvm.dylib                        0x000000011d949988 TVMFuncCall + 72\\n  [bt] (2) 3   libtvm.dylib                        0x000000011d985c0d std::__1::__function::__func<tvm::runtime::RPCModuleNode::WrapRemote(void*)::'lambda'(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), std::__1::allocator<tvm::runtime::RPCModuleNode::WrapRemote(void*)::'lambda'(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)>, void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)>::operator()(tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&) + 93\\n  [bt] (1) 2   libtvm.dylib                        0x000000011d98a856 tvm::runtime::RPCSession::CallFunc(void*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void* (*)(int, tvm::runtime::TVMArgValue const&), tvm::runtime::PackedFunc const*) + 310\"),), error_no=4, all_cost=10.226943254470825, timestamp=1587486943.3877)\t[('tile_y', [-1, 16]), ('tile_x', [-1, 1]), ('tile_k', [-1, 1])],None,4\n",
    "No: 5\tGFLOPS: 49.93/70.62\tresult: MeasureResult(costs=(0.0430116264,), error_no=0, all_cost=0.9452111721038818, timestamp=1587486947.725454)\t[('tile_y', [-1, 8]), ('tile_x', [-1, 16]), ('tile_k', [-1, 16])],None,531\n",
    "No: 6\tGFLOPS: 21.08/70.62\tresult: MeasureResult(costs=(0.1018855638,), error_no=0, all_cost=1.9517230987548828, timestamp=1587486949.593319)\t[('tile_y', [-1, 128]), ('tile_x', [-1, 4]), ('tile_k', [-1, 8])],None,392\n",
    "No: 7\tGFLOPS: 30.83/70.62\tresult: MeasureResult(costs=(0.069658234,), error_no=0, all_cost=4.6122565269470215, timestamp=1587486950.92193)\t[('tile_y', [-1, 256]), ('tile_x', [-1, 1024]), ('tile_k', [-1, 32])],None,723\n",
    "No: 8\tGFLOPS: 17.86/70.62\tresult: MeasureResult(costs=(0.1202257842,), error_no=0, all_cost=2.2247397899627686, timestamp=1587486953.050207)\t[('tile_y', [-1, 256]), ('tile_x', [-1, 64]), ('tile_k', [-1, 1])],None,74\n",
    "No: 9\tGFLOPS: 12.10/70.62\tresult: MeasureResult(costs=(0.17751151,), error_no=0, all_cost=2.986374616622925, timestamp=1587486956.2598379)\t[('tile_y', [-1, 2]), ('tile_x', [-1, 8]), ('tile_k', [-1, 2])],None,155\n",
    "No: 10\tGFLOPS: 31.15/70.62\tresult: MeasureResult(costs=(0.068937509,), error_no=0, all_cost=1.3671948909759521, timestamp=1587486957.581886)\t[('tile_y', [-1, 4]), ('tile_x', [-1, 256]), ('tile_k', [-1, 2])],None,211\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning job finishes, we can apply history best from the log file and check its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt4: 0.088083\n"
     ]
    }
   ],
   "source": [
    "# apply history best from log file\n",
    "with autotvm.apply_history_best('matmul.log'):\n",
    "    with tvm.target.create(\"llvm\"):\n",
    "        s, arg_bufs = matmul_v1(M, K, N)\n",
    "        func = tvm.build(s, arg_bufs)\n",
    "\n",
    "# check correctness\n",
    "a_np = np.random.uniform(size=(M, K)).astype(np.float32)\n",
    "b_np = np.random.uniform(size=(K, N)).astype(np.float32)\n",
    "c_np = a_np.dot(b_np)\n",
    "\n",
    "a_tvm = tvm.nd.array(a_np)\n",
    "b_tvm = tvm.nd.array(b_np)\n",
    "c_tvm = tvm.nd.empty(c_np.shape)\n",
    "func(a_tvm, b_tvm, c_tvm)\n",
    "\n",
    "tvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)\n",
    "\n",
    "evaluator = func.time_evaluator(func.entry_name, ctx, number=10)\n",
    "print('Opt: %f' % evaluator(a_tvm, b_tvm, c_tvm).mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tvm)",
   "language": "python",
   "name": "tvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
