{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial to TVM\n",
    "============\n",
    "\n",
    "TVM is a framework allows you to write high tensor kernels productively.\n",
    "The big idea of TVM's productivity is to decouple kernel description and code organization.\n",
    "Before actually diving into the usage of TVM, we first show how to install TVM.\n",
    "\n",
    "Installation\n",
    "-------------\n",
    "\n",
    "If you are using AWS Sagemake Notebook with CUDA 10 installed, [@icelemon9](https://github.com/icemelon9/) provides a pip package:\n",
    "````\n",
    "pip install https://haichen-tvm.s3-us-west-2.amazonaws.com/tvm_cu100-0.6.dev0-cp36-cp36m-linux_x86_64.whl\n",
    "pip install https://haichen-tvm.s3-us-west-2.amazonaws.com/topi-0.6.dev0-py3-none-any.whl\n",
    "````\n",
    "\n",
    "If you are running a different environment or want full control on installation, a tutorial to building from source is below. First, TVM depends on:\n",
    "0. TVM sticks on Python 3(.5), [conda](https://www.anaconda.com/distribution/) is recommended.\n",
    "1. Make sure your `cmake` version is older than *3.2* to succeccfully generate build files.\n",
    "2. TVM uses on LLVM JIT to run CPU codes. Download a proper LLVM binary distribution [here](http://releases.llvm.org/download.html). 6.x and 8.x are recommended (Versions older than 4.x is not supported and 6.x has problem on binary compatibility with gcc).\n",
    "3. For GPU users, CUDA should be installed.\n",
    "\n",
    "After prerequisites are resolved, let's clone the repo and build it.\n",
    "````\n",
    "git clone --recursive https://github.com/dmlc/tvm.git && cd tvm\n",
    "mkdir build && cd build\n",
    "touch config.cmake # cmake/config.cmake\n",
    "echo \"set(USE_LLVM path/to/llvm-config)\" >> config.cmake\n",
    "echo \"set(USE_CUDA path/to/cuda)\" >> config.cmake # GPU users only\n",
    "cmake ..\n",
    "make -j`nproc`\n",
    "````\n",
    "\n",
    "After build is done, you should\n",
    "````\n",
    "export PYTHONPATH=\"$HOME/tvm-dev/tvm/python\":$PYTHONPATH\n",
    "export PYTHONPATH=\"$HOME/tvm-dev/tvm/topi/python\":$PYTHONPATH\n",
    "````\n",
    "\n",
    "Then, let's start a quick tour to TVM.\n",
    "\n",
    "Op\n",
    "----\n",
    "In TVM, every tensor is an `Op` node. An `Op` has its own inputs, outputs, and semantics.\n",
    "Writing TVM applications is essentially to depict the relationship among op's.\n",
    "In this tutorial, we will instruct you how to write `ComputeOp`'s and a `HybridOp`'s.\n",
    "\n",
    "ComputeOp\n",
    "--------------\n",
    "\n",
    "As it is aforementioned, the key of enabling both high performance and productivity is to decouple kernel description and code organization. In this section, we first demonstrate how to depict computation, then show how to manipulate the decoupled code organization.\n",
    "\n",
    "### A simple vector addition\n",
    "We first declare the input tensors. The value of each dimension of the shape can either be a constant or a symbolic expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "\n",
    "# You can also make a TVM symbolic shape.\n",
    "# We leave this as an exercise.\n",
    "# Hint: Try `var_n = tvm.var('n')`, and put `var_n` into argument list too.\n",
    "vec_n = 128\n",
    "\n",
    "# placeholder(shape: tuple, dtype: str, name: str)\n",
    "vec_a = tvm.placeholder((vec_n, ), dtype='float32', name='a')\n",
    "vec_b = tvm.placeholder((vec_n, ), dtype='float32', name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the vectors are declared, we can now depict the computation.\n",
    "Most of the deep learning kernels, are equations/formulae wrapped by levels of loops.\n",
    "For example, \"vector addition\" code looks like:\n",
    "```` C\n",
    "for (int x = 0; x < n; ++x)\n",
    "  c[x] = a[x] + b[x];\n",
    "````\n",
    "Thus, in TVM, it allows you to depict computations by simply specifying the shape and the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute(shape: tuple, fcompute: callable)\n",
    "vec_c = tvm.compute((vec_n, ), lambda x: vec_a[x] + vec_b[x], name='c')\n",
    "\n",
    "# Write a outer product! Try 2-D!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once compute description is done, we can compile this Op by creating a schedule for this Op and compile this Op.\n",
    "TVM provides two interfaces of building: one for sanity check and the other for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce c {\n",
      "  for (x, 0, 128) {\n",
      "    c[x] = (a[x] + b[x])\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch = tvm.create_schedule(vec_c.op)\n",
    "\n",
    "# schedule, argument list, options\n",
    "# simple_mode indicates this lowering is for sanity check\n",
    "# The function compiled by TVM uses a side-effect style.\n",
    "# The output tensor is also in the argument list instead of being a return value.\n",
    "ir = tvm.lower(sch, [vec_a, vec_b, vec_c], simple_mode=True)\n",
    "\n",
    "print(ir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schedule provides interfaces for you to manipulate the code organization. A well-tuned code organization can provide orders of magnitute acceleration comparing against the vanilla code. Meanwhile, it aggressively refactor the code, which also aggressively break the readability, and modularity.\n",
    "\n",
    "To make the code organization clear TVM provides `split`, `unroll`, `vectorize`, `reorder`, and etc. premitives for you to play to the code organization. The example below demonstrates the usage of both `split` and `vectorization`. Please refer [TVM schedule premitives](https://docs.tvm.ai/tutorials/language/schedule_primitives.html#sphx-glr-tutorials-language-schedule-primitives-py) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce c {\n",
      "  for (x.outer, 0, 16) {\n",
      "    c[ramp((x.outer*8), 1, 8)] = (a[ramp((x.outer*8), 1, 8)] + b[ramp((x.outer*8), 1, 8)])\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#axis, reduce_axis\n",
    "x = vec_c.op.axis[0]\n",
    "\n",
    "xo, xi = sch[vec_c].split(x, 8)\n",
    "\n",
    "sch[vec_c].vectorize(xi)\n",
    "\n",
    "ir = tvm.lower(sch, [vec_a, vec_b, vec_c], simple_mode=True)\n",
    "\n",
    "print(ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule, argument list, options\n",
    "# This API will call lower with simple_mode=False\n",
    "module = tvm.build(sch, [vec_a, vec_b, vec_c], target='llvm')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "npa = np.arange(128).astype('float32')\n",
    "npb = np.arange(128).astype('float32')\n",
    "\n",
    "nda = tvm.ndarray.array(npa)\n",
    "ndb = tvm.ndarray.array(npb)\n",
    "ndc = tvm.ndarray.array(np.zeros((128, ), dtype='float32'))\n",
    "\n",
    "module(nda, ndb, ndc)\n",
    "\n",
    "# Test the results\n",
    "tvm.testing.assert_allclose(ndc.asnumpy(), np.arange(128).astype('float32') * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about matrix multiplication?\n",
    "Compared to vector add, a key difference of matrix multiplication is \"reduction\".\n",
    "To write a reduction Op, we first need to define the reduction domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, k, n = 128, 64, 128\n",
    "\n",
    "red = tvm.reduce_axis((0, k), name='k')\n",
    "mat_a = tvm.placeholder((m, k), dtype='float32', name='mat_a')\n",
    "mat_b = tvm.placeholder((k, n), dtype='float32', name='mat_b')\n",
    "\n",
    "# axis can be a single red (1d), or an array of red (2d or more).\n",
    "# Writing 2d convolution as exercise.\n",
    "mat_c = tvm.compute((m, n), lambda x, y: tvm.sum(mat_a[x, red] * mat_b[red, y], axis=red), name='mat_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing GeMM is beyond the scope of this tutorial. Refer [this tutorial](https://docs.tvm.ai/tutorials/optimize/opt_gemm.html) for more details, if interested. Here we only demonstrate the sanity of building this `Op`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce mat_c {\n",
      "  for (x, 0, 128) {\n",
      "    for (y, 0, 128) {\n",
      "      mat_c[((x*128) + y)] = 0f\n",
      "      for (k, 0, 64) {\n",
      "        mat_c[((x*128) + y)] = (mat_c[((x*128) + y)] + (mat_a[((x*64) + k)]*mat_b[((k*128) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch = tvm.create_schedule(mat_c.op)\n",
    "\n",
    "print(tvm.lower(sch, [mat_a, mat_b, mat_c], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together\n",
    "\n",
    "Recall that in TVM everything is an Op and every Op is a tensor. Thus, actually `vec_c` and `mat_c` can also interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tvm.compute((m, n), lambda x, y: mat_c[x, y] * vec_c[x], name='d')\n",
    "\n",
    "sch = tvm.create_schedule(d.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at the vanilla IR. In this case `vec_c` and `mat_c` becomes intermediate results, so whether put them in the argument list depends if you need these results (they both are correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [mat_c] storage_scope = \"global\"\n",
      "allocate mat_c[float32 * 16384]\n",
      "// attr [c] storage_scope = \"global\"\n",
      "allocate c[float32 * 128]\n",
      "produce mat_c {\n",
      "  for (x, 0, 128) {\n",
      "    for (y, 0, 128) {\n",
      "      mat_c[((x*128) + y)] = 0f\n",
      "      for (k, 0, 64) {\n",
      "        mat_c[((x*128) + y)] = (mat_c[((x*128) + y)] + (mat_a[((x*64) + k)]*mat_b[((k*128) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce c {\n",
      "  for (x, 0, 128) {\n",
      "    c[x] = (a[x] + b[x])\n",
      "  }\n",
      "}\n",
      "produce d {\n",
      "  for (x, 0, 128) {\n",
      "    for (y, 0, 128) {\n",
      "      d[((x*128) + y)] = (mat_c[((x*128) + y)]*c[x])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(sch, [vec_a, vec_b, mat_a, mat_b, d], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we introduce an important concept Op fusion: actually we do not need to hold the whole `mat_c` in memory, we can compute it when necessary. TVM provides 2 premitives `compute_at` and `compute_inline`.\n",
    "\n",
    "`compute_inline` totally fuses the an op into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [mat_c] storage_scope = \"global\"\n",
      "allocate mat_c[float32 * 16384]\n",
      "produce mat_c {\n",
      "  for (x, 0, 128) {\n",
      "    for (y, 0, 128) {\n",
      "      mat_c[((x*128) + y)] = 0f\n",
      "      for (k, 0, 64) {\n",
      "        mat_c[((x*128) + y)] = (mat_c[((x*128) + y)] + (mat_a[((x*64) + k)]*mat_b[((k*128) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce d {\n",
      "  for (x, 0, 128) {\n",
      "    for (y, 0, 128) {\n",
      "      d[((x*128) + y)] = (mat_c[((x*128) + y)]*(a[x] + b[x]))\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch[vec_c].compute_inline()\n",
    "print(tvm.lower(sch, [vec_a, vec_b, mat_a, mat_b, d], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mat_c` is an reduction op, so it cannot be inlined. We can still apply `compute_at` to it, which indicated we only partially compute the elements of tensor `mat_c` required by `d`'s `axis[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// attr [mat_c] storage_scope = \"global\"\n",
      "allocate mat_c[float32 * 1]\n",
      "produce d {\n",
      "  for (x, 0, 128) {\n",
      "    for (y, 0, 128) {\n",
      "      produce mat_c {\n",
      "        mat_c[0] = 0f\n",
      "        for (k, 0, 64) {\n",
      "          mat_c[0] = (mat_c[0] + (mat_a[((x*64) + k)]*mat_b[((k*128) + y)]))\n",
      "        }\n",
      "      }\n",
      "      d[((x*128) + y)] = (mat_c[0]*(a[x] + b[x]))\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch[mat_c].compute_at(sch[d], d.op.axis[1])\n",
    "print(tvm.lower(sch, [vec_a, vec_b, mat_a, mat_b, d], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: actually here, using `sch[vec_c].compute_at(sch[d], d.op.axis[1])` is the same as `compute_inline` for LLVM code generation. `compute_inline` is only to make the HalideIR looks nicer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Op\n",
    "------------\n",
    "As it is mentioned in last section, compute op is dedicated for Op's that are formulae wrapped by levels of loops. When it comes to some irregular kernels, you still need a general tool to do this. In TVM, we have Hybrid Script, which allows you to simply write a subset of Python, and this Python can be lowered to TVM Op's.\n",
    "\n",
    "To use this functionality, we first need to write a Python function and annotate it with a `tvm.hybrid.script` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.hybrid.script\n",
    "def foo(n, a, b):\n",
    "    # Do not worry about this undeclared function, the decorator already handles it for you!\n",
    "    c = output_tensor((n, ), a.dtype)\n",
    "    for i in range(n):\n",
    "        if a[i] > 100:\n",
    "            c[i] = a[i] + b[i]\n",
    "        else:\n",
    "            c[i] = a[i] - b[i]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then this function has two modes: software emulation and `Op` compilation. It is simply an overload. If you pass actual tensors `list`, `tuple`, or `np.array` it will emulate the behaviour of software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa = np.arange(128).astype('float32')\n",
    "npb = np.arange(128).astype('float32')\n",
    "\n",
    "# npa and npb are numpy arrays, so it does software emulation.\n",
    "out = foo(128, npa, npb)\n",
    "ref = np.array([0] * 101 + list(range(202, 256, 2))).astype('float32')\n",
    "\n",
    "# We can check the results.\n",
    "tvm.testing.assert_allclose(out, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass TVM tensors to this function, it will do Op compilation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for (i, 0, 128) {\n",
      "  if ((a(i) > 100f)) {\n",
      "    c(i) =(a(i) + b(i))\n",
      "  } else {\n",
      "    c(i) =(a(i) - b(i))\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = tvm.placeholder((128, ), dtype='float32', name='a')\n",
    "b = tvm.placeholder((128, ), dtype='float32', name='b')\n",
    "\n",
    "# Note: if you want to pass a constant to the compilation,\n",
    "# you should convert it to TVM symbolic.\n",
    "# `tvm.convert` is an intelligent function, which can differentiate\n",
    "# the data type and convert it to corresponding TVM data structure.\n",
    "c_foo = foo(tvm.convert(128), a, b)\n",
    "\n",
    "print(c_foo.op.body)\n",
    "\n",
    "sch = tvm.create_schedule(c_foo.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use exactly the same APIs to play with this schedule. From loop organization to target compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nda = tvm.ndarray.array(npa)\n",
    "ndb = tvm.ndarray.array(npb)\n",
    "ndc = tvm.ndarray.array(np.zeros((128, ), dtype='float32'))\n",
    "\n",
    "# Since `n` is replaced by a constant, you no longer need to pass `n` as a argument.\n",
    "module = tvm.build(sch, [a, b, c_foo], target='llvm')\n",
    "module(nda, ndb, ndc)\n",
    "tvm.testing.assert_allclose(ndc.asnumpy(), ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full syntax feature supported by Hybrid Script, please refer [the language manual](https://docs.tvm.ai/langref/hybrid_script.html).\n",
    "\n",
    "Hybrid Script is not perfect, there may be bugs. If you cannot find a solution after carefully reading the dumped stack trace, feel free to post on [the forum](https://discuss.tvm.ai) and @me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
