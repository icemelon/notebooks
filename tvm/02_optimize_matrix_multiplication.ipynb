{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to optimize GEMM on CPU\n",
    "\n",
    "In this tutorial, we will demonstrate how to use TVM to optimize square matrix multiplication on CPU.\n",
    "\n",
    "There are two important optimizations on intense computation applications executed on CPU:\n",
    "- Increase the cache hit rate of memory access. Both complex numerical computation and hot-spot memory access can be accelerated from high cache hit rate. This requires us to transform the origin memory access pattern to the pattern fits the cache policy.\n",
    "- SIMD (Single instruction multi-data), or we call it vector processing unit. Every time, a small batch of data, rather than a single grid, will be processed. This requires us to transform the data access pattern in the loop body in uniform pattern so that the LLVM backend can lower it to SIMD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "We first import TVM and write a baseline implementation for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tvm\n",
    "from tvm import te\n",
    "\n",
    "# The size of the matrix\n",
    "# (M, K) x (K, N)\n",
    "# You are free to try out different shapes, sometimes TVM optimization outperforms numpy with MKL.\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "# The default tensor type in tvm\n",
    "dtype = \"float32\"\n",
    "\n",
    "# using Intel AVX2(Advanced Vector Extensions) ISA for SIMD\n",
    "# To get the best performance, please change the following line\n",
    "# to llvm -mcpu=core-avx2, or specific type of CPU you use\n",
    "target = 'llvm -mcpu=core-avx2'\n",
    "ctx = tvm.context(target, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the compute, or the algorithm, of matrix multiplication first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = te.placeholder((M, K), name='A')\n",
    "B = te.placeholder((K, N), name='B')\n",
    "k = te.reduce_axis((0, K), 'k')\n",
    "C = te.compute(\n",
    "           (M, N),\n",
    "           lambda y, x: te.sum(A[y, k] * B[k, x], axis=k),\n",
    "           name='C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TVM, we can always inspect lower level IR to debug or optimize our schedule. Here is the generated IR using our baseline schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimFunc([A, B, C]) attrs={\"tir.noalias\": (bool)1, \"global_symbol\": \"main\"} {\n",
      "  for (y, 0, 1024) {\n",
      "    for (x, 0, 1024) {\n",
      "      C[((y*1024) + x)] = 0f\n",
      "      for (k, 0, 1024) {\n",
      "        C[((y*1024) + x)] = (C[((y*1024) + x)] + (A[((y*1024) + k)]*B[((k*1024) + x)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default schedule\n",
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVM also provides profiler to measure the kernel latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 4.860180 sec\n"
     ]
    }
   ],
   "source": [
    "# Random generated tensor for testing\n",
    "a = tvm.nd.array(numpy.random.rand(M, K).astype(dtype), ctx)\n",
    "b = tvm.nd.array(numpy.random.rand(K, N).astype(dtype), ctx)\n",
    "c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), ctx)\n",
    "\n",
    "func = tvm.build(s, [A, B, C], target=target, name='mmult')\n",
    "func(a, b, c)\n",
    "\n",
    "evaluator = func.time_evaluator(func.entry_name, ctx, number=1)\n",
    "print('Baseline: %f sec' % evaluator(a, b, c).mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking\n",
    "\n",
    "A important trick to enhance the cache hit rate is blocking â€” data chunk will be computed block by block. The memory access inside the block is a small neighbourhood which is with high memory locality. In this tutorial, I picked up 32 as the blocking factor. So the block will fill 32 * 32 * sizeof(float) which is 4KB in the cache whose total size is 32KB (L1 data cache)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt1: 0.224869\n"
     ]
    }
   ],
   "source": [
    "bn = 32\n",
    "s = te.create_schedule(C.op)\n",
    "\n",
    "# Get the iteration axis from the operator\n",
    "y, x = s[C].op.axis\n",
    "k, = s[C].op.reduce_axis\n",
    "\n",
    "# Blocking by loop tiling\n",
    "yo, yi = s[C].split(y, bn)\n",
    "xo, xi = s[C].split(x, bn)\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "# Hoist reduction domain outside the blocking loop\n",
    "s[C].reorder(yo, xo, ko, ki, yi, xi)\n",
    "\n",
    "func = tvm.build(s, [A, B, C], target=target, name='mmult')\n",
    "func(a, b, c)\n",
    "\n",
    "# By simply tiling the loop 32x32, and hoisting ko, ki outside the blocking loops,\n",
    "# we can see big speedup compared with the baseline.\n",
    "evaluator = func.time_evaluator(func.entry_name, ctx, number=10)\n",
    "print('Opt1: %f' % evaluator(a, b, c).mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimFunc([A, B, C]) attrs={\"tir.noalias\": (bool)1, \"global_symbol\": \"main\"} {\n",
      "  for (y.outer, 0, 32) {\n",
      "    for (x.outer, 0, 32) {\n",
      "      for (y.inner.init, 0, 32) {\n",
      "        for (x.inner.init, 0, 32) {\n",
      "          C[((((y.outer*32768) + (y.inner.init*1024)) + (x.outer*32)) + x.inner.init)] = 0f\n",
      "        }\n",
      "      }\n",
      "      for (k.outer, 0, 256) {\n",
      "        for (k.inner, 0, 4) {\n",
      "          for (y.inner, 0, 32) {\n",
      "            for (x.inner, 0, 32) {\n",
      "              C[((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)) + x.inner)] = (C[((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)) + x.inner)] + (A[((((y.outer*32768) + (y.inner*1024)) + (k.outer*4)) + k.inner)]*B[((((k.outer*4096) + (k.inner*1024)) + (x.outer*32)) + x.inner)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Another important trick is vectorization. When the memory access pattern is uniform, the compiler can detect this pattern and pass the continuous memory to vector processor. In TVM, we can use vectorize interface to hint the compiler this pattern, so that we can accelerate it vastly.\n",
    "\n",
    "In this tutorial, we chose to vectorize the inner loop row data since it is cache friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt2: 0.244345\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "s[C].vectorize(xi)\n",
    "\n",
    "func = tvm.build(s, [A, B, C], target=target, name='mmult')\n",
    "func(a, b, c)\n",
    "\n",
    "evaluator = func.time_evaluator(func.entry_name, ctx, number=10)\n",
    "print('Opt2: %f' % evaluator(a, b, c).mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimFunc([A, B, C]) attrs={\"tir.noalias\": (bool)1, \"global_symbol\": \"main\"} {\n",
      "  for (y.outer, 0, 32) {\n",
      "    for (x.outer, 0, 32) {\n",
      "      for (y.inner.init, 0, 32) {\n",
      "        C[ramp((((y.outer*32768) + (y.inner.init*1024)) + (x.outer*32)), 1, 32)] = x32(0f)\n",
      "      }\n",
      "      for (k.outer, 0, 256) {\n",
      "        for (k.inner, 0, 4) {\n",
      "          for (y.inner, 0, 32) {\n",
      "            C[ramp((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)), 1, 32)] = (C[ramp((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)), 1, 32)] + (x32(A[((((y.outer*32768) + (y.inner*1024)) + (k.outer*4)) + k.inner)])*B[ramp((((k.outer*4096) + (k.inner*1024)) + (x.outer*32)), 1, 32)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Permutation\n",
    "\n",
    "If we look at the above IR, we can see the inner loop row data is vectorized and B is transformed into PackedB. The traversal of PackedB is sequential now. So we will look at the access pattern of A. In current schedule, A is accessed column by column which is not cache friendly. If we change the nested loop order of ki and inner axes xi, the access pattern for A matrix is more cache friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt3: 0.066508\n"
     ]
    }
   ],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "y, x = s[C].op.axis\n",
    "k, = s[C].op.reduce_axis\n",
    "yo, xo, yi, xi = s[C].tile(y, x, bn, bn)\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "# re-ordering\n",
    "s[C].reorder(yo, xo, ko, yi, ki, xi)\n",
    "s[C].vectorize(xi)\n",
    "\n",
    "func = tvm.build(s, [A, B, C], target=target, name='mmult')\n",
    "func(a, b, c)\n",
    "\n",
    "evaluator = func.time_evaluator(func.entry_name, ctx, number=10)\n",
    "print('Opt3: %f' % evaluator(a, b, c).mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimFunc([A, B, C]) attrs={\"tir.noalias\": (bool)1, \"global_symbol\": \"main\"} {\n",
      "  for (y.outer, 0, 32) {\n",
      "    for (x.outer, 0, 32) {\n",
      "      for (y.inner.init, 0, 32) {\n",
      "        C[ramp((((y.outer*32768) + (y.inner.init*1024)) + (x.outer*32)), 1, 32)] = x32(0f)\n",
      "      }\n",
      "      for (k.outer, 0, 256) {\n",
      "        for (y.inner, 0, 32) {\n",
      "          for (k.inner, 0, 4) {\n",
      "            C[ramp((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)), 1, 32)] = (C[ramp((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)), 1, 32)] + (x32(A[((((y.outer*32768) + (y.inner*1024)) + (k.outer*4)) + k.inner)])*B[ramp((((k.outer*4096) + (k.inner*1024)) + (x.outer*32)), 1, 32)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel\n",
    "\n",
    "Futhermore, we can also utilize multi-core processors to do the thread-level parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt4: 0.041917\n"
     ]
    }
   ],
   "source": [
    "# parallel\n",
    "s[C].parallel(yo)\n",
    "\n",
    "func = tvm.build(s, [A, B, C], target=target, name='mmult')\n",
    "func(a, b, c)\n",
    "\n",
    "evaluator = func.time_evaluator(func.entry_name, ctx, number=10)\n",
    "print('Opt4: %f' % evaluator(a, b, c).mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrimFunc([A, B, C]) attrs={\"tir.noalias\": (bool)1, \"global_symbol\": \"main\"} {\n",
      "  parallel (y.outer, 0, 32) {\n",
      "    for (x.outer, 0, 32) {\n",
      "      for (y.inner.init, 0, 32) {\n",
      "        C[ramp((((y.outer*32768) + (y.inner.init*1024)) + (x.outer*32)), 1, 32)] = x32(0f)\n",
      "      }\n",
      "      for (k.outer, 0, 256) {\n",
      "        for (y.inner, 0, 32) {\n",
      "          for (k.inner, 0, 4) {\n",
      "            C[ramp((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)), 1, 32)] = (C[ramp((((y.outer*32768) + (y.inner*1024)) + (x.outer*32)), 1, 32)] + (x32(A[((((y.outer*32768) + (y.inner*1024)) + (k.outer*4)) + k.inner)])*B[ramp((((k.outer*4096) + (k.inner*1024)) + (x.outer*32)), 1, 32)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tvm)",
   "language": "python",
   "name": "tvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
